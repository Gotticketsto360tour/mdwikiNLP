# Class 6: Scikit-learn and Vectorization

Note: Changes to this plan may occur (but the required stuff will not change)


### TL:DR
 - Required:
   - Have your best model ready from last time
   - If you didn't do so for the last class split up in your study group and watch one video each on a machine learning method and meet back up and briefly present the model to each other. See the link in the subsection on class 5.
   - Watch [this short introduction](https://www.youtube.com/watch?v=WN18JksF9Cg) to TF-IDF using scikit-learn.
 - Highly recommended:
   - Watch this video on [PCA](https://www.youtube.com/watch?v=FgakZw6K1QQ)
 - Recommended:
   - Skim the documentation for [CountVectorizers](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) and [TfidfVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer)

During last class we only breifly touched upon text vectorization. The operation of turning text into vectors. Today we will look a bit more into this. Most notably the TF-IDF, which we briefly touched upon during our first class.

---

## Plan for Class
* Examples of vectorization using n-grams, binary, and stopwords
* Introduction to TF-IDF
  * Mathematical derivation
  * its link to information theory
* (if we have time) Dimensionality reduction using SVD or PCA

---

---

## Materials used in Class
Will be up later.


<!--
* one-hot
* tf-idf
-->
